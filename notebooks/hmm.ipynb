{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hmmlearn\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import os\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFreqDictPerScore(directory_path):\n",
    "    score_dict = {}\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        if not files:\n",
    "            continue\n",
    "        score = root.split(\"/\")[1]\n",
    "        freq_dict = {}\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                seq = content.split(\" \")\n",
    "                for s in seq:\n",
    "                    if s not in freq_dict:\n",
    "                        freq_dict[s] = 1\n",
    "                    else:\n",
    "                        freq_dict[s] += 1\n",
    "        score_dict[score] = freq_dict\n",
    "    return score_dict\n",
    "\n",
    "score_dict = getFreqDictPerScore(\"essay_hmm\")\n",
    "for s, freq in score_dict.items():\n",
    "    print(s, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFreqDict(directory_path):\n",
    "    freq_dict = {}\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                seq = content.split(\" \")\n",
    "                for s in seq:\n",
    "                    if s not in freq_dict:\n",
    "                        freq_dict[s] = 1\n",
    "                    else:\n",
    "                        freq_dict[s] += 1\n",
    "    return freq_dict\n",
    "\n",
    "freq_dict = getFreqDict(\"essay_hmm\")\n",
    "filtered_dict = {key: value for key, value in freq_dict.items() if value >= 10965}\n",
    "filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##reading from essay_hmm and generating a word dictionary for str -> int\n",
    "def setWordDict(directory_path, word_dict):\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                seq = content.split(\" \")\n",
    "                for s in seq:\n",
    "                    if s not in word_dict and s in filtered_dict:\n",
    "                        word_dict[s] = len(word_dict) \n",
    "    return word_dict\n",
    "word_dict = {}\n",
    "word_dict = setWordDict(\"essay_hmm\", word_dict)\n",
    "print(len(word_dict),word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##convert the essay into int and get the max seq number\n",
    "def convertStrToIntEssay(directory_path, word_dict):\n",
    "    max_seq, max_file = 0, \"\"\n",
    "    essay_inputs = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            essay_id = file.split(\".\")[0]\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                seq = content.split(\" \")\n",
    "                essay_int = [word_dict[s] for s in seq if s in word_dict and s in filtered_dict]\n",
    "                # max_seq = max(max_seq, len(essay_int))\n",
    "                if len(essay_int) > max_seq:\n",
    "                    max_seq, max_file = len(essay_int), file\n",
    "            essay_inputs.append((essay_id, essay_int))\n",
    "    return max_seq, max_file, essay_inputs\n",
    "\n",
    "##max_13257\n",
    "max_seq, max_file, essay_inputs = convertStrToIntEssay(\"essay_hmm\", word_dict)\n",
    "##list of tuple (essay_id, seq)\n",
    "print(max_seq,max_file, len(essay_inputs[0][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##keep duplicating itself until it reaches the max length\n",
    "##essay_inputs is in [(essay_id, seq)] form\n",
    "def padding_essay(essay_inputs, max_seq):\n",
    "    for i, (file_name, seq) in enumerate(essay_inputs):\n",
    "        while len(seq) < max_seq:\n",
    "            seq.extend(seq)\n",
    "        seq = seq[0:max_seq]\n",
    "        essay_inputs[i] = (file_name, seq)\n",
    "\n",
    "padding_essay(essay_inputs, max_seq)\n",
    "all(len(tup[1]) == max_seq for tup in essay_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##keep duplicating itself until it reaches the max length\n",
    "##content is in \"seqKseqK\" string form\n",
    "def convertStrToIntSet(file_path, word_dict):\n",
    "    set_inputs = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        seqs = content.split(\"K\")\n",
    "        # print(len(seqs))\n",
    "        for seq in seqs:\n",
    "            # print(seq)\n",
    "            seq_int = []\n",
    "            for s in seq.split(\" \"):\n",
    "                if len(s) == 0 or s not in word_dict or s not in filtered_dict:\n",
    "                    continue\n",
    "                seq_int.append(word_dict[s])\n",
    "            set_inputs.append(seq_int)\n",
    "    return set_inputs\n",
    "\n",
    "##2d array that contains seq \n",
    "score_set = []\n",
    "##train input is a 2d array. First len is number of doc, second is sequence of doc\n",
    "# for s in [\"1.0\", \"1.5\", \"2.0\", \"2.5\", \"3.0\", \"4.5\", \"5.0\", \"5.5\",\"3.5\", \"4.0\", \"0.5\", \"6.0\"]:\n",
    "for s in [\"4.0\"]:\n",
    "    train_seqs = convertStrToIntSet(\"train/\" + s + \".txt\", word_dict)\n",
    "    test_seqs = convertStrToIntSet(\"test/\" + s + \".txt\", word_dict)\n",
    "    # print(len(train_seqs))\n",
    "    score_set.append((s, train_seqs, test_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, train_seqs, test_seqs in score_set:\n",
    "    print(s, len(train_seqs), len(test_seqs), len(train_seqs[0]), len(test_seqs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##keep duplicating itself until it reaches the max length\n",
    "def padding_set(set_inputs, max_seq):\n",
    "    for i, seq in enumerate(set_inputs):\n",
    "        while len(seq) < max_seq:\n",
    "            seq.extend(seq)\n",
    "        seq = seq[0:max_seq]\n",
    "        set_inputs[i] = seq\n",
    "# def padding_set(set_inputs, max_seq):\n",
    "#     for i, seq in enumerate(set_inputs):\n",
    "#         seq = np.pad(seq, (0, max_seq - len(seq)), constant_values=len(word_dict))\n",
    "#         set_inputs[i] = seq[0:1000]\n",
    "##calling padding\n",
    "for s, train_seqs, test_seqs in score_set:\n",
    "    padding_set(train_seqs, max_seq)\n",
    "    padding_set(test_seqs, max_seq)\n",
    "\n",
    "##checking if all has same length\n",
    "for s, train_seqs, test_seqs in score_set:\n",
    "    print(s, len(train_seqs), len(test_seqs), all(len(seq) == max_seq for seq in train_seqs), all(len(seq) == max_seq for seq in test_seqs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, train_seqs, test_seqs in score_set:\n",
    "    print(s, len(train_seqs), len(test_seqs), len(train_seqs[0]), len(test_seqs[0]), np.array(train_seqs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, train_seqs, test_seqs in score_set[0:1]:\n",
    "    print(np.unique(np.array(train_seqs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestModel(n_fits, N, train_input, test_input, seed):\n",
    "    train_input = train_input + test_input\n",
    "    best_score = best_model = None\n",
    "    for idx in range(n_fits):\n",
    "        model = hmm.CategoricalHMM(\n",
    "        n_components=N, random_state=seed, \n",
    "        init_params='se')  # don't init transition, set it below\n",
    "    # we need to initialize with random transition matrix probabilities\n",
    "    # because the default is an even likelihood transition\n",
    "    # we know transitions are rare (otherwise the casino would get caught!)\n",
    "    # so let's have an Dirichlet random prior with an alpha value of\n",
    "    # (0.1, 0.9) to enforce our assumption transitions happen roughly 10%\n",
    "    # of the time\n",
    "\n",
    "    ##A\n",
    "        epsilon = 1e-2\n",
    "        transmat = np.full((N, N), 1 / N) + epsilon\n",
    "        transmat /= transmat.sum(axis=1, keepdims=True)\n",
    "        random_noise = np.random.randn(N, N) * 1e-2  # Adjust the scale of the noise as needed\n",
    "        transmat += random_noise\n",
    "        transmat /= transmat.sum(axis=1, keepdims=True)\n",
    "        # print(transmat)\n",
    "    # model.transmat_ = np.array([np.random.dirichlet([0.9, 0.1]),\n",
    "    #                             np.random.dirichlet([0.1, 0.9])])\n",
    "        model.transmat_ = transmat\n",
    "        model.fit(train_input)\n",
    "        score = model.score(train_input)\n",
    "        print(f'{seed} Model-{s} #{idx}\\tScore: {score}')\n",
    "        if best_score is None or score > best_score:\n",
    "            best_model = model\n",
    "            best_score = score\n",
    "    return best_score, best_model\n",
    "\n",
    "n_fits = 1\n",
    "N = 2\n",
    "for s, train_input, test_input in score_set:\n",
    "    lowest_best_score, lowest_best_model = findBestModel(n_fits, N, train_input, test_input, N)\n",
    "    if not os.path.exists(\"model/\" + s):\n",
    "                os.makedirs(\"model/\" + s)\n",
    "    with open(\"model/\" + s + \"/\" + s + \"-\" + str(N) + \"-model\" + str(lowest_best_score) + \".pkl\", \"wb\") as file:\n",
    "            pickle.dump(lowest_best_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lowest_best_model.transmat_)\n",
    "print(lowest_best_model.emissionprob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# For this example, we will model the stages of a conversation,\n",
    "# where each sentence is \"generated\" with an underlying topic, \"cat\" or \"dog\"\n",
    "states = [\"cat\", \"dog\"]\n",
    "id2topic = dict(zip(range(len(states)), states))\n",
    "# we are more likely to talk about cats first\n",
    "start_probs = np.array([0.6, 0.4])\n",
    "\n",
    "# For each topic, the probability of saying certain words can be modeled by\n",
    "# a distribution over vocabulary associated with the categories\n",
    "\n",
    "vocabulary = [\"tail\", \"fetch\", \"mouse\", \"food\"]\n",
    "# if the topic is \"cat\", we are more likely to talk about \"mouse\"\n",
    "# if the topic is \"dog\", we are more likely to talk about \"fetch\"\n",
    "emission_probs = np.array([[0.25, 0.1, 0.4, 0.25],\n",
    "                           [0.2, 0.5, 0.1, 0.2]])\n",
    "\n",
    "# Also assume it's more likely to stay in a state than transition to the other\n",
    "trans_mat = np.array([[0.8, 0.2], [0.2, 0.8]])\n",
    "\n",
    "\n",
    "# Pretend that every sentence we speak only has a total of 5 words,\n",
    "# i.e. we independently utter a word from the vocabulary 5 times per sentence\n",
    "# we observe the following bag of words (BoW) for 8 sentences:\n",
    "observations = [[\"tail\", \"mouse\", \"mouse\", \"food\", \"mouse\"],\n",
    "        [\"food\", \"mouse\", \"mouse\", \"food\", \"mouse\"],\n",
    "        [\"tail\", \"mouse\", \"mouse\", \"tail\", \"mouse\"],\n",
    "        [\"food\", \"mouse\", \"food\", \"food\", \"tail\"],\n",
    "        [\"tail\", \"fetch\", \"mouse\", \"food\", \"tail\"],\n",
    "        [\"tail\", \"fetch\", \"fetch\", \"food\", \"fetch\"],\n",
    "        [\"fetch\", \"fetch\", \"fetch\", \"food\", \"tail\"],\n",
    "        [\"food\", \"mouse\", \"food\", \"food\", \"tail\"],\n",
    "        [\"tail\", \"mouse\", \"mouse\", \"tail\", \"mouse\"],\n",
    "        [\"fetch\", \"fetch\", \"fetch\", \"fetch\", \"fetch\"]]\n",
    "\n",
    "# Convert \"sentences\" to numbers:\n",
    "vocab2id = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "def sentence2counts(sentence):\n",
    "    ans = []\n",
    "    for word, idx in vocab2id.items():\n",
    "        count = sentence.count(word)\n",
    "        ans.append(count)\n",
    "    return ans\n",
    "\n",
    "X = []\n",
    "for sentence in observations:\n",
    "    row = sentence2counts(sentence)\n",
    "    X.append(row)\n",
    "\n",
    "data = np.array(X, dtype=int)\n",
    "\n",
    "# pretend this is repeated, so we have more data to learn from:\n",
    "lengths = [len(X)]*5\n",
    "sequences = np.tile(data, (5,1))\n",
    "print(np.array(sequences).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = [[0.5], [1.0], [-1.0], [0.42], [0.24]]\n",
    "X2 = [[2.4], [4.2], [0.5], [-0.24]]\n",
    "X = np.concatenate([X1, X2])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "num_workers = 10\n",
    "n_fits = 1\n",
    "N = 5\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    for s, train_input, test_input in score_set:\n",
    "        # arguments = [(n_fits, N, train_input, test_input)] * num_workers\n",
    "        arguments = [(n_fits, N, train_input, test_input, i) for i in range(num_workers)]\n",
    "        futures = [executor.submit(findBestModel, *arg) for arg in arguments]\n",
    "        concurrent.futures.wait(futures)\n",
    "        result = [future.result() for future in futures]\n",
    "        print(result)\n",
    "        best_result = min(result, key=lambda x: x[0])\n",
    "        lowest_best_score, lowest_best_model = best_result\n",
    "        if not os.path.exists(\"model/\" + s):\n",
    "                os.makedirs(\"model/\" + s)\n",
    "        with open(\"model/\" + s + \"/\" + s + \"-\" + str(N) + \"-model\" + str(lowest_best_score) + \".pkl\", \"wb\") as file:\n",
    "            pickle.dump(lowest_best_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateHMMScoreOnEssay(model_path, essay_inputs):\n",
    "    ##read model\n",
    "    model_list = []\n",
    "    scores = [ \"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\", \"3.0\",\"3.5\", \"4.0\",\"4.5\", \"5.0\", \"5.5\",  \"6.0\"]\n",
    "    result = []\n",
    "    for score in scores:\n",
    "        model_file = glob.glob(os.path.join(model_path, score, '*.pkl'))[0]\n",
    "        with open(model_file, \"rb\") as file:\n",
    "            model = pickle.load(file)\n",
    "            model_list.append((score,model))\n",
    "    for i, (essay_id, essay_input) in enumerate(essay_inputs):\n",
    "        hmm_result = [essay_id]\n",
    "        for s, model in model_list:\n",
    "            print(f\"{i}-Training essay {essay_id} against model-{s}\")\n",
    "            hmm_score = model.score([essay_input])\n",
    "            hmm_result.append(hmm_score)\n",
    "            # hmm_result.append(score)\n",
    "        result.append(hmm_result)\n",
    "    return scores, result\n",
    "\n",
    "scores, hmm_result = generateHMMScoreOnEssay(\"model\",essay_inputs[0:10])\n",
    "hmm_result_df = pd.DataFrame(hmm_result)\n",
    "col = ['id'] + scores\n",
    "hmm_result_df.columns = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_result_df.to_csv(\"essay_hmm_reult.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##reading from the text file for observation sequnce. Only using 4.0 for now\n",
    "\n",
    "# def readInputFromFile(score):\n",
    "#     file_path = score\n",
    "#     with open(file_path, \"r\") as file:\n",
    "#         lines = file.readlines()\n",
    "#         seqs = lines[0].split(\"K\")\n",
    "#         words = \" \".join(seqs).split(\" \")\n",
    "#         string_id = {word: i for i, word in enumerate(set(words))}\n",
    "\n",
    "#         seqs = [s.split(\" \") for s in seqs]\n",
    "#         ##convert words in seqs to numbers\n",
    "#         seqs = [[string_id[w] for w in s] for s in seqs]\n",
    "#         # lines = file.readlines()\n",
    "#         # print(len(lines))\n",
    "#         # string_to_id = {string: i for i, string in enumerate(set(lines[0]))}\n",
    "#         # raw_input = lines[0].split(\"K\")\n",
    "#         # print(len(raw_input))\n",
    "#         # raw_string_pre_pad = [s.split(\" \") for s in raw_input ]\n",
    "#         #\n",
    "#         # max_seq = max(len(s) for s in seqs)\n",
    "#         # hmm_input = [np.pad(s, (0, max_seq - len(s)), constant_values=9999) for s in seqs]\n",
    "#         hmm_input = seqs\n",
    "#     return hmm_input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
